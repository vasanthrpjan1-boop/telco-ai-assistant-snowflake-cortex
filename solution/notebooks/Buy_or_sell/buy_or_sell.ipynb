{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "bli7nnxxvrhi44nuntgo",
   "authorId": "3234949799362",
   "authorName": "BECKY2",
   "authorEmail": "becky.oconnor@snowflake.com",
   "sessionId": "de3641b1-eb28-4c42-8e8d-f84639cd0d1a",
   "lastEditTime": 1738833911790
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9ba374-cd85-415e-b3e5-a9e862c9d395",
   "metadata": {
    "name": "title",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# What do we know about Snowflake?"
  },
  {
   "cell_type": "code",
   "id": "2f5349a5-4153-47a2-b060-6de09c65e1cf",
   "metadata": {
    "language": "sql",
    "name": "look_data_market_place",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE VIEW DATA.STOCK_PRICES \n\nAS \n\nSELECT \n\n\nTICKER,\nASSET_CLASS,\nPRIMARY_EXCHANGE_CODE,\nPRIMARY_EXCHANGE_NAME,\nDATE,\n\"'all-day_high'\" ALL_DAY_HIGH,\n\"'all-day_low'\" ALL_DAY_LOW,\n\"'nasdaq_volume'\" NASDAQ_VOLUME,\n\"'post-market_close'\" POST_MARKET_CLOSE,\n\"'pre-market_open'\" PRE_MARKET_OPEN,\nYEAR(DATE)::text YEAR,\nMONTHNAME(DATE)MONTHNAME, MONTH(DATE) MONTHNO FROM (\nSELECT * EXCLUDE VARIABLE_NAME\nFROM FINANCE__ECONOMICS.CYBERSYN.STOCK_PRICE_TIMESERIES)\n\n\nPIVOT (SUM(value) FOR VARIABLE IN (ANY ORDER BY VARIABLE));\nSELECT * FROM DATA.STOCK_PRICES WHERE TICKER = 'SNOW'    LIMIT 5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b285932-0664-4a60-8da5-d4dbc400e1a0",
   "metadata": {
    "name": "SNOWFLAKE_SHARE_PRICES",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## 01 - Market Place Shareprice Trends\nAll this is sstraight out the box from the Market Place"
  },
  {
   "cell_type": "code",
   "id": "894a2c7d-a324-4517-a878-d6bdc7b26677",
   "metadata": {
    "language": "python",
    "name": "VIZ_SHARE_PRICES",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\nfrom snowflake.snowpark.functions import *\nfrom snowflake.snowpark.types import *\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nst.markdown('#### SNOWFLAKE STOCK MARKET PATTERNS')\n\nsdate = st.number_input('Choose Year:',2020,2024,2024)\nstock_table = session.table('FINANCE__ECONOMICS.CYBERSYN.STOCK_PRICE_TIMESERIES').filter(year('DATE')==sdate)\n\ndef stock_metric(variable, ticker):\n    stocks = stock_table.filter((col('TICKER')==ticker) & (col('VARIABLE')==variable))\n    stocks = stocks.group_by(col('DATE')).agg(min('VALUE').alias('VALUE')).order_by('DATE')\n    return st.line_chart(stocks, y='VALUE',x='DATE', color = '#29B5E8')\n\n\nticker = st.selectbox('Select Ticker:',stock_table.select(col('TICKER')).distinct().to_pandas())\n\ncol1,col2,col3 = st.columns(3)\n\nwith col1:\n    st.markdown('#### ALL DAY HIGH')\n    stock_metric('all-day_high',ticker)\nwith col2:\n    st.markdown('#### ALL DAY LOW')\n    stock_metric('all-day_low',ticker)\nwith col3:\n    st.markdown('#### NASDAQ VOLUME')\n    stock_metric('nasdaq_volume',ticker)\n\ncol1,col2 = st.columns(2)\nwith col1:\n    st.markdown('#### PRE MARKET OPEN')\n    stock_metric('pre-market_open',ticker)\nwith col2:\n    st.markdown('#### POST MARKET CLOSE')\n    stock_metric('post-market_close',ticker)\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4f3241c-f525-407e-be14-17df3b901476",
   "metadata": {
    "name": "heading_list_pdfs",
    "collapsed": false,
    "resultHeight": 127
   },
   "source": "## 02 - Structure the Unstructured\nAnalyst reports are in a variety of shapes and sizes - via pdf documents - we may however switch to document AI to build a model which we can use to take the data out which we need"
  },
  {
   "cell_type": "code",
   "id": "643a6bb6-d83e-49ad-b950-6c9c52ce27a5",
   "metadata": {
    "language": "sql",
    "name": "list_pdfs",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "select BUILD_SCOPED_FILE_URL('@DATA.PDFS',RELATIVE_PATH), * from directory(@DATA.PDFS)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1203b983-0dbd-44c0-937a-4c5fab4f42d5",
   "metadata": {
    "name": "process_docs",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Process Documents from Document AI"
  },
  {
   "cell_type": "code",
   "id": "33812c37-dbcb-46c0-88be-c3c37dd471c6",
   "metadata": {
    "language": "sql",
    "name": "call_document_ai_model",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE TABLE if not exists DOCUMENT_AI_PROCESSED AS\nSELECT\n\n*,\n\nSNOWFLAKE_BUY_OR_SELL.DOCUMENT_AI.SNOWFLAKE_BUY_SELL!PREDICT(GET_PRESIGNED_URL(@DATA.PDFS,RELATIVE_PATH),2) DOC_META \nFROM DIRECTORY(@DATA.PDFS);\n\nSELECT * EXCLUDE (MD5, ETAG) FROM DOCUMENT_AI_PROCESSED",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b416cc59-2b3e-4d8f-b3a9-8ac283eb9f68",
   "metadata": {
    "language": "sql",
    "name": "view_meta_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS REPORTS_STRUCTURED AS \nselect RELATIVE_PATH,\n\nDOC_META:__documentMetadata:ocrScore OCR_SCORE,\nDOC_META:RATING[0]:value::text RATING,\nDOC_META:MARKET_PRICE[0]:value::text MARKET_PRICE,\nDOC_META:DATE_REPORT[0]:value::text DATE_REPORT,\nDOC_META:NAME_OF_REPORT_PROVIDER[0]:value::text NAME_OF_REPORT_PROVIDER\n\n\n\n\nfrom DOCUMENT_AI_PROCESSED;\n\nSELECT * FROM REPORTS_STRUCTURED",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf1a0c7b-8ed2-4fa1-acf0-b02ba5b51123",
   "metadata": {
    "name": "heading_extract_data",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## Extract ALL text from Analyst reports data\nWe will use this to make the data searchable."
  },
  {
   "cell_type": "code",
   "id": "4b896256-167f-40d2-a99d-a796b36f8ff8",
   "metadata": {
    "language": "sql",
    "name": "extract_data",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS DATA.REPORTS AS \n\nselect * exclude layout from (\n\nSELECT *, \n\n    build_stage_file_url(@DATA.PDFS,RELATIVE_PATH) URL,\n    \n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n                                '@DATA.PDFS',\n                                RELATIVE_PATH,\n                                {'mode': 'LAYOUT'} )  AS LAYOUT, LAYOUT:content::text CONTENT, LAYOUT:metadata:pageCount PAGE_COUNT\n        \n                                    FROM DIRECTORY (@DATA.PDFS));\n\nCREATE OR REPLACE VIEW REPORTS_ALL_DATA AS\n\nselect A.*, B.CONTENT from REPORTS_STRUCTURED A INNER JOIN \nREPORTS B ON \n\nA.RELATIVE_PATH = B.RELATIVE_PATH;\n\n\nSELECT * FROM REPORTS_ALL_DATA",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f3be0ad-f93e-4513-9675-1663b2ab5a34",
   "metadata": {
    "language": "python",
    "name": "pdf_viewer",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n\n\n\nst.title(\"Equity Research Reports\")\nsession = get_active_session()\n\nside_letters = session.table('SNOWFLAKE_BUY_OR_SELL.DATA.REPORTS_ALL_DATA').select('RELATIVE_PATH').filter(F.col('RELATIVE_PATH').like('ANALYST_REPORTS%'))\nfile_id = st.selectbox('Select Report:', side_letters)\ndoc_details = session.table('SNOWFLAKE_BUY_OR_SELL.DATA.REPORTS_ALL_DATA').filter(F.col('RELATIVE_PATH')==file_id).limit(1)\ndoc_detailsspd = doc_details.to_pandas()\n\n\nst.markdown('#### Report Details')\ncol1,col2 = st.columns(2)\n\nwith col1:\n    st.markdown(f'''__Report Date:__ {doc_detailsspd.DATE_REPORT.iloc[0]}''')\n    st.markdown(f'''__Research Firm:__ {doc_detailsspd.NAME_OF_REPORT_PROVIDER.iloc[0]}''')\n    \nwith col2:\n    st.markdown(f'''__Close Price Value:__ {doc_detailsspd.MARKET_PRICE.iloc[0]}''')\n    st.markdown(f'''__Recommendation:__ {doc_detailsspd.RATING.iloc[0]}''')\n\n# New Section \nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as snow_funcs\n\nimport pypdfium2 as pdfium\nfrom datetime import datetime\n\n# Write directly to the app\n\n\ndoc_ai_context = \"SNOWFLAKE_BUY_OR_SELL.DATA\"\ndoc_ai_source_table = \"REPORTS\"\ndoc_ai_source_verify_table = \"REPORTS_ALL_DATA\"\ndoc_ai_doc_stage = \"PDFS\"\n\n# Dict that has the name of the columns that needs to be verified, it has the column name of the column \n# with value and column with the score\nvalue_dict = {\n    \"OPERATOR_VALUE\": {\n        \"VAL_COL\": \"OPERATOR_VALUE\",\n        \"SCORE_COL\": \"OPERATOR_SCORE\"\n    }\n}\n\n# The minimum score needed to not be verified\nthreshold_score = 0.5\n\n# HELPER FUNCTIONS\n# Function to generate filter to only get the rows that are missing values or have a score below the threshold\ndef generate_filter(col_dict:dict,  score_val:float): #score_cols:list, score_val:float, val_cols:list):\n    \n    filter_exp = ''\n\n    # For each column\n    for col in col_dict:\n        # Create the filter on score threashold or missing value\n        if len(filter_exp) > 0:\n                filter_exp += ' OR '\n        filter_exp += f'(({col_dict[col][\"SCORE_COL\"]} <= {score_val} ) OR ({col_dict[col][\"VAL_COL\"]} IS NULL))'\n\n    if len(filter_exp) > 0:\n       filter_exp = f'({filter_exp}) AND ' \n    \n    # Filter out documents already verified\n    filter_exp  += 'verification_date is null'\n    return filter_exp\n\n# Generates a column list for counting the number of documents that is missing values or a score less that the threashold\n# by each column\ndef count_missing_select(col_dict:dict, score_val:float):\n    select_list = []\n\n    for col in col_dict:\n        col_exp = (snow_funcs.sum(\n                          snow_funcs.iff(\n                                    (\n                                        (snow_funcs.col(col_dict[col][\"VAL_COL\"]).is_null())\n                                        | \n                                        (snow_funcs.col(col_dict[col][\"SCORE_COL\"]) <= score_val)\n                                    ), 1,0\n                              )\n                      ).as_(col)\n                )\n        select_list.append(col_exp)\n        \n    return select_list\n\n# Function to display a pdf page\ndef display_pdf_page():\n    pdf = st.session_state['pdf_doc']\n    page = pdf[st.session_state['pdf_page']]\n            \n    bitmap = page.render(\n                    scale = 8, \n                    rotation = 0,\n            )\n    pil_image = bitmap.to_pil()\n    st.image(pil_image)\n\n# Function to move to the next PDF page\ndef next_pdf_page():\n    if st.session_state.pdf_page + 1 >= len(st.session_state['pdf_doc']):\n        st.session_state.pdf_page = 0\n    else:\n        st.session_state.pdf_page += 1\n\n# Function to move to the previous PDF page\ndef previous_pdf_page():\n    if st.session_state.pdf_page > 0:\n        st.session_state.pdf_page -= 1\n\n# Function to get the name of all documents that need verification\ndef get_documents(doc_df):\n    \n    lst_docs = [dbRow[0] for dbRow in doc_df.collect()]\n    # Add a default None value\n    lst_docs.insert(0, None)\n    return lst_docs\n\n# MAIN\n\n# Get the table with all documents with extracted values\ndf_agreements = session.table(f\"{doc_ai_context}.{doc_ai_source_table}\")\n\n# Get the documents we already gave verified\ndf_validated_docs = session.table(f\"{doc_ai_context}.{doc_ai_source_verify_table}\")\n\n# Join\ndf_all_docs = df_agreements.join(df_validated_docs,on='RELATIVE_PATH', how='left', lsuffix = '_L', rsuffix = '_R')\n\n# Filter out all document that has missing values of score below the threasholds\nvalidate_filter = generate_filter(value_dict, threshold_score)\ndf_validate_docs = df_all_docs.filter(validate_filter)\n#col1, col2 = st.columns(2)\n#col1.metric(label=\"Total Documents\", value=df_agreements.count())\n#col2.metric(label=\"Documents Needing Validation\", value=df_validate_docs.count())\n\n# Get the number of documents by value that needs verifying\n#select_list = count_missing_select(value_dict, threshold_score)\n#df_verify_counts = df_validate_docs.select(select_list)\n#verify_cols = df_verify_counts.columns\n\n#st.subheader(\"Number of documents needing validation by extraction value\")\n#st.bar_chart(data=df_verify_counts.unpivot(\"needs_verify\", \"check_col\", verify_cols), x=\"CHECK_COL\", y=\"NEEDS_VERIFY\")\n\n# Verification section\nst.divider()\ncol1, col2 = st.columns(2)\nwith col1:\n    st.markdown('#### RAW PDF STORED IN FILE STORE')\n    with st.container():\n        # If we have selected a document\n        if file_id:        \n        # Display the extracted values\n            df_doc = df_validate_docs.filter(snow_funcs.col(\"FILE_NAME\") == file_id)\n            if 'pdf_page' not in st.session_state:\n                st.session_state['pdf_page'] = 0\n            if 'pdf_url' not in st.session_state:\n                st.session_state['pdf_url'] = file_id    \n            if 'pdf_doc' not in st.session_state or st.session_state['pdf_url'] != file_id:\n                pdf_stream = session.file.get_stream(f\"@{doc_ai_context}.{doc_ai_doc_stage}/{file_id}\")\n                pdf = pdfium.PdfDocument(pdf_stream)\n                st.session_state['pdf_doc'] = pdf\n                st.session_state['pdf_url'] = file_id\n                st.session_state['pdf_page'] = 0\n                \n            nav_col1, nav_col2, nav_col3 = st.columns(3)\n            with nav_col1:\n                if st.button(\"⏮️ Previous\", on_click=previous_pdf_page):\n                    pass    \n                with nav_col2:\n                    st.write(f\"page {st.session_state['pdf_page'] +1} of {len(st.session_state['pdf_doc'])} pages\")\n                with nav_col3:\n                    if st.button(\"Next ⏭️\", on_click=next_pdf_page):\n                        pass\n        \n    \n    \n            display_pdf_page()\n    with col2:\n        st.markdown('#### EXTRACTED TEXT FROM PDFS')\n        with st.container():\n            st.markdown(doc_detailsspd.CONTENT.iloc[0])\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51dfa611-fbb1-430c-8103-8589d00d3a5b",
   "metadata": {
    "name": "BROWSE_DOCS",
    "collapsed": false,
    "resultHeight": 113
   },
   "source": "### Chunk Documents to make the search service easier to find out information\nYou will see that the document parsing retains all the structure - making it easier to chop up into chunks for searching.  Also, it is fully aware of tables/paragraphs and headings"
  },
  {
   "cell_type": "code",
   "id": "41c73bc9-f938-4eb5-8522-c8309e1dae34",
   "metadata": {
    "language": "sql",
    "name": "chunked_table",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE  TABLE IF NOT EXISTS DATA.CHUNKED  AS \n\nSELECT \nRELATIVE_PATH,\nRATING,\nDATE_REPORT,\nNAME_OF_REPORT_PROVIDER,\nSPLIT_PART(RELATIVE_PATH,'/',1)::array DOCUMENT_TYPE,\nSPLIT_PART(RELATIVE_PATH,'/',2)::TEXT DOCUMENT, \n\n\n\nVALUE::TEXT TEXT FROM REPORTS_ALL_DATA,LATERAL FLATTEN( SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(CONTENT,'markdown',500,20));\n\nSELECT * FROM DATA.CHUNKED LIMIT 4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a636914d-f38a-4bfe-8a00-50bb40634e6e",
   "metadata": {
    "name": "header_quarter_field",
    "collapsed": false,
    "resultHeight": 47
   },
   "source": "#### Add A column to create a financial quarter field"
  },
  {
   "cell_type": "code",
   "id": "65170746-33e0-4d38-b671-c145b02e3b08",
   "metadata": {
    "language": "sql",
    "name": "CHUNKED_WITH_TIME",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS DATA.ANALYST_REPORTS_CHUNKED AS\n\nSELECT *, REPLACE(SNOWFLAKE.CORTEX.COMPLETE('mistral-large', \nconcat('look at the following file name and return only Q2FY25 or Q3FY25.  September is in Q2 and november is in Q3', RELATIVE_PATH,'only return the answer in a format like this - Q3FY25. do not reurn comments' )),' ','')::ARRAY PERIOD, \nSNOWFLAKE.CORTEX.SENTIMENT(TEXT) SENTIMENT\n\n\nFROM DATA.CHUNKED",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "326eb127-64b5-4c5b-b6ba-7b5d9a09e564",
   "metadata": {
    "language": "sql",
    "name": "VIEW_ALL_CHUNKED",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM DATA.ANALYST_REPORTS_CHUNKED;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc5a9c4e-7447-454f-ad07-2a2e0c16a8d0",
   "metadata": {
    "name": "cell5",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### BRING IN THE SOUND DATA"
  },
  {
   "cell_type": "code",
   "id": "d032d9cc-748f-46e0-ad71-53ce177acc7b",
   "metadata": {
    "language": "sql",
    "name": "shared_transcripts",
    "resultHeight": 439,
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM DATA.SUMMARY_TEXT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "872417b8-d830-42f6-87ef-a6d1c707293b",
   "metadata": {
    "language": "sql",
    "name": "sound_data",
    "collapsed": false,
    "resultHeight": 182
   },
   "outputs": [],
   "source": "CREATE VIEW IF NOT EXISTS DATA.EARNINGS_CALLS_FORMATTED AS \nSELECT RELATIVE_PATH, 'NA' RATING, 'NA' DATE_REPORT, 'NA' NAME_OF_REPORT_PROVIDER,  ['EARNINGS_CALLS'] DOCUMENT_TYPE,RELATIVE_PATH DOCUMENT, \nTEXT,REPLACE(REPLACE(REPLACE(REPLACE(RELATIVE_PATH,'EARNINGS_',''),'.mp3',''),'_',''),'FY20','FY')::ARRAY PERIOD, SENTIMENT FROM DATA.SUMMARY_TEXT;\n\nSELECT * FROM DATA.EARNINGS_CALLS_FORMATTED LIMIT 3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "898dbe79-1555-4691-a2b7-cee1f144d10f",
   "metadata": {
    "language": "sql",
    "name": "union_text_sound",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS DATA.TEXT_AND_SOUND AS \n\nselect * from DATA.EARNINGS_CALLS_FORMATTED\n\nUNION ALL\n\nSELECT * FROM DATA.ANALYST_REPORTS_CHUNKED",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac9b52fb-4b1b-4c82-be72-6ece4c19b892",
   "metadata": {
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### CREATE A SEARCH SERVICE"
  },
  {
   "cell_type": "code",
   "id": "0495d240-6d11-4faf-be9a-85eb05f0c92c",
   "metadata": {
    "language": "sql",
    "name": "viewing_single_chunk",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE  CORTEX SEARCH SERVICE STREAMLIT_AND_CORTEX.CHUNKED_REPORTS\n  ON TEXT\n  ATTRIBUTES DOCUMENT_TYPE,PERIOD\n  WAREHOUSE = BUILD_UK_WAREHOUSE\n  TARGET_LAG = '1 hour'\n  COMMENT = 'SEARCH SERVICE FOR REPORTS - CHUNKED'\n  AS SELECT * FROM DATA.TEXT_AND_SOUND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63ee9eb8-5a47-4c68-b533-cb0c24080b9d",
   "metadata": {
    "language": "sql",
    "name": "view_search_service",
    "resultHeight": 112,
    "collapsed": false
   },
   "outputs": [],
   "source": "USE SCHEMA STREAMLIT_AND_CORTEX;\nSHOW CORTEX SEARCH SERVICES",
   "execution_count": null
  }
 ]
}