{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "w66bdu5ihzxrbqneayim",
   "authorId": "2656583713627",
   "authorName": "USER",
   "authorEmail": "becky.oconnor@snowflake.com",
   "sessionId": "f4567743-ef97-4297-9e22-e24c67f0034e",
   "lastEditTime": 1741281765845
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9ba374-cd85-415e-b3e5-a9e862c9d395",
   "metadata": {
    "name": "title",
    "collapsed": false,
    "resultHeight": 74
   },
   "source": "# What do we know about Snowflake?\n\n## Section 3 - Cortex Analyst\nReview the stock information by running the following code below"
  },
  {
   "cell_type": "code",
   "id": "2f5349a5-4153-47a2-b060-6de09c65e1cf",
   "metadata": {
    "language": "sql",
    "name": "look_data_market_place",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE VIEW DEFAULT_SCHEMA.STOCK_PRICES \n\nAS \n\nSELECT \n\n\nTICKER,\nASSET_CLASS,\nPRIMARY_EXCHANGE_CODE,\nPRIMARY_EXCHANGE_NAME,\nDATE,\n\"'all-day_high'\" ALL_DAY_HIGH,\n\"'all-day_low'\" ALL_DAY_LOW,\n\"'nasdaq_volume'\" NASDAQ_VOLUME,\n\"'post-market_close'\" POST_MARKET_CLOSE,\n\"'pre-market_open'\" PRE_MARKET_OPEN,\nYEAR(DATE)::text YEAR,\nMONTHNAME(DATE)MONTHNAME, MONTH(DATE) MONTHNO FROM (\nSELECT * EXCLUDE VARIABLE_NAME\nFROM FINANCE__ECONOMICS.CYBERSYN.STOCK_PRICE_TIMESERIES)\n\n\nPIVOT (SUM(value) FOR VARIABLE IN (ANY ORDER BY VARIABLE));\nSELECT * FROM DEFAULT_SCHEMA.STOCK_PRICES WHERE TICKER = 'SNOW'    LIMIT 5",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b285932-0664-4a60-8da5-d4dbc400e1a0",
   "metadata": {
    "name": "SNOWFLAKE_SHARE_PRICES",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## 3a - Analysing Market Place Shareprice Trends the traditional way\nAll this is straight out the box from the Market Place.  Run the cell below to see how this looks like as a Streamlit dashboard"
  },
  {
   "cell_type": "code",
   "id": "894a2c7d-a324-4517-a878-d6bdc7b26677",
   "metadata": {
    "language": "python",
    "name": "VIZ_SHARE_PRICES",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nimport pandas as pd\n\nfrom snowflake.snowpark.functions import *\nfrom snowflake.snowpark.types import *\n\n# We can also use Snowpark for our analyses!\nfrom snowflake.snowpark.context import get_active_session\nsession = get_active_session()\n\nst.markdown('#### SNOWFLAKE STOCK MARKET PATTERNS')\n\nsdate = st.number_input('Choose Year:',2020,2024,2024)\nstock_table = session.table('FINANCE__ECONOMICS.CYBERSYN.STOCK_PRICE_TIMESERIES').filter(year('DATE')==sdate)\n\ndef stock_metric(variable, ticker):\n    stocks = stock_table.filter((col('TICKER')==ticker) & (col('VARIABLE')==variable))\n    stocks = stocks.group_by(col('DATE')).agg(min('VALUE').alias('VALUE')).order_by('DATE')\n    return st.line_chart(stocks, y='VALUE',x='DATE', color = '#29B5E8')\n\n\nticker = st.selectbox('Select Ticker:',stock_table.select(col('TICKER')).distinct().to_pandas())\n\ncol1,col2,col3 = st.columns(3)\n\nwith col1:\n    st.markdown('#### ALL DAY HIGH')\n    stock_metric('all-day_high',ticker)\nwith col2:\n    st.markdown('#### ALL DAY LOW')\n    stock_metric('all-day_low',ticker)\nwith col3:\n    st.markdown('#### NASDAQ VOLUME')\n    stock_metric('nasdaq_volume',ticker)\n\ncol1,col2 = st.columns(2)\nwith col1:\n    st.markdown('#### PRE MARKET OPEN')\n    stock_metric('pre-market_open',ticker)\nwith col2:\n    st.markdown('#### POST MARKET CLOSE')\n    stock_metric('post-market_close',ticker)\n\nst.markdown('''You will see that you can get lots of information in a dashboard - streamlit makes this capabilty very flexible.\n\nHowever, some users want to be able to answer ad-hoc questions without having to create a new\ndashboard or waiting for a new improved dashboard to be built.  This is where **Cortex Analyst** comes into play\n\nPlase go to to Step 3 in the lab instructions to find out how **Cortex Analyst** works''')",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b4f3241c-f525-407e-be14-17df3b901476",
   "metadata": {
    "name": "heading_list_pdfs",
    "collapsed": false,
    "resultHeight": 127
   },
   "source": "## Section 4 - document AI\n## 4b - Processing PDFs at scale\n\nYou should now have built a model called **Analyst Reports** in document AI.  Now, the next stage is to process multiple documents using the model which you have built.\n\nRun the query below to view all the PDFs which are currently residing in a Snowflake Stage\n"
  },
  {
   "cell_type": "code",
   "id": "643a6bb6-d83e-49ad-b950-6c9c52ce27a5",
   "metadata": {
    "language": "sql",
    "name": "list_pdfs",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "select BUILD_SCOPED_FILE_URL('@DOCUMENT_AI.ANALYST_REPORTS',RELATIVE_PATH), * from directory(@DOCUMENT_AI.ANALYST_REPORTS)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1203b983-0dbd-44c0-937a-4c5fab4f42d5",
   "metadata": {
    "name": "process_docs",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### Process Documents from Document AI\nYou will now use the model previously created to process these documents.  Each document will produce meta data under the column name **DOC_META** - this will consist of all the fields that was built in the model"
  },
  {
   "cell_type": "code",
   "id": "33812c37-dbcb-46c0-88be-c3c37dd471c6",
   "metadata": {
    "language": "sql",
    "name": "call_document_ai_model",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE TABLE if not exists DOCUMENT_AI.DOCUMENT_AI_PROCESSED AS\nSELECT\n\n*,\n\nDOCUMENT_AI.ANALYST_REPORTS_1!PREDICT(GET_PRESIGNED_URL(@DOCUMENT_AI.ANALYST_REPORTS,RELATIVE_PATH),1) DOC_META \nFROM DIRECTORY(@DOCUMENT_AI.ANALYST_REPORTS);\n\nSELECT * EXCLUDE (MD5, ETAG) FROM DOCUMENT_AI_PROCESSED",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "69e140c7-9ee5-46de-b5c4-971d620b7fb5",
   "metadata": {
    "name": "Format_the_view",
    "collapsed": false
   },
   "source": "Let's now make this more readable as a structured table"
  },
  {
   "cell_type": "code",
   "id": "b416cc59-2b3e-4d8f-b3a9-8ac283eb9f68",
   "metadata": {
    "language": "sql",
    "name": "view_meta_data",
    "collapsed": false,
    "codeCollapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS DOCUMENT_AI.REPORTS_STRUCTURED AS \nselect RELATIVE_PATH,\n\nDOC_META:__documentMetadata:ocrScore OCR_SCORE,\nDOC_META:RATING[0]:value::text RATING,\nDOC_META:MARKET_PRICE[0]:value::text MARKET_PRICE,\nDOC_META:DATE_REPORT[0]:value::text DATE_REPORT,\nDOC_META:NAME_OF_REPORT_PROVIDER[0]:value::text NAME_OF_REPORT_PROVIDER\n\n\n\n\nfrom DOCUMENT_AI_PROCESSED;\n\nSELECT * FROM DOCUMENT_AI.REPORTS_STRUCTURED",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "bf1a0c7b-8ed2-4fa1-acf0-b02ba5b51123",
   "metadata": {
    "name": "heading_extract_data",
    "collapsed": false,
    "resultHeight": 102
   },
   "source": "## Extract ALL text from Analyst reports data\nWe will use this to make the data searchable.  Document AI is good for taking out **key facts** about the data.  Cortex Parse Document extracts all the text - you can then later use this to make it searchable or run it through other AI function such as **sentiment**, **Summarize** or **Classification**.\n\nRun the following SQL below to parse the document\n\nClick [here](https://docs.snowflake.com/en/user-guide/snowflake-cortex/parse-document) for more information on Cortex Parse Document"
  },
  {
   "cell_type": "code",
   "id": "4b896256-167f-40d2-a99d-a796b36f8ff8",
   "metadata": {
    "language": "sql",
    "name": "extract_data",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS DOCUMENT_AI.REPORTS AS \n\nselect * exclude layout from (\n\nSELECT *, \n\n    build_stage_file_url(@DOCUMENT_AI.ANALYST_REPORTS,RELATIVE_PATH) URL,\n    \n    SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n                                '@DOCUMENT_AI.ANALYST_REPORTS',\n                                RELATIVE_PATH,\n                                {'mode': 'LAYOUT'} )  AS LAYOUT, LAYOUT:content::text CONTENT, LAYOUT:metadata:pageCount PAGE_COUNT\n        \n                                    FROM DIRECTORY (@DOCUMENT_AI.ANALYST_REPORTS));\n\nCREATE OR REPLACE VIEW DOCUMENT_AI.REPORTS_ALL_DATA AS\n\nselect A.*, B.CONTENT from DOCUMENT_AI.REPORTS_STRUCTURED A INNER JOIN \nREPORTS B ON \n\nA.RELATIVE_PATH = B.RELATIVE_PATH;\n\n\nSELECT * FROM DOCUMENT_AI.REPORTS_ALL_DATA",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "4f3be0ad-f93e-4513-9675-1663b2ab5a34",
   "metadata": {
    "language": "python",
    "name": "pdf_viewer",
    "collapsed": false,
    "resultHeight": 0,
    "codeCollapsed": true
   },
   "outputs": [],
   "source": "# Import python packages\nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as F\nfrom snowflake.snowpark import types as T\n\n\n\nst.title(\"Equity Research Reports\")\nsession = get_active_session()\n\nside_letters = session.table('DOCUMENT_AI.REPORTS_ALL_DATA').select('RELATIVE_PATH')#.filter(F.col('RELATIVE_PATH').like('ANALYST_REPORTS%'))\nfile_id = st.selectbox('Select Report:', side_letters)\ndoc_details = session.table('DOCUMENT_AI.REPORTS_ALL_DATA').limit(1)\ndoc_detailsspd = doc_details.to_pandas()\n\n\nst.markdown('#### Report Details')\ncol1,col2 = st.columns(2)\n\nwith col1:\n    st.markdown(f'''__Report Date:__ {doc_detailsspd.DATE_REPORT.iloc[0]}''')\n    st.markdown(f'''__Research Firm:__ {doc_detailsspd.NAME_OF_REPORT_PROVIDER.iloc[0]}''')\n    \nwith col2:\n    st.markdown(f'''__Close Price Value:__ {doc_detailsspd.MARKET_PRICE.iloc[0]}''')\n    st.markdown(f'''__Recommendation:__ {doc_detailsspd.RATING.iloc[0]}''')\n\n# New Section \nimport streamlit as st\nfrom snowflake.snowpark.context import get_active_session\nfrom snowflake.snowpark import functions as snow_funcs\n\nimport pypdfium2 as pdfium\nfrom datetime import datetime\n\n# Write directly to the app\n\n\ndoc_ai_context = \"DATAOPS_EVENT_PROD.DOCUMENT_AI\"\ndoc_ai_source_table = \"REPORTS\"\ndoc_ai_source_verify_table = \"REPORTS_ALL_DATA\"\ndoc_ai_doc_stage = \"ANALYST_REPORTS\"\n\n# Dict that has the name of the columns that needs to be verified, it has the column name of the column \n# with value and column with the score\nvalue_dict = {\n    \"OPERATOR_VALUE\": {\n        \"VAL_COL\": \"OPERATOR_VALUE\",\n        \"SCORE_COL\": \"OPERATOR_SCORE\"\n    }\n}\n\n# The minimum score needed to not be verified\nthreshold_score = 0.5\n\n# HELPER FUNCTIONS\n# Function to generate filter to only get the rows that are missing values or have a score below the threshold\ndef generate_filter(col_dict:dict,  score_val:float): #score_cols:list, score_val:float, val_cols:list):\n    \n    filter_exp = ''\n\n    # For each column\n    for col in col_dict:\n        # Create the filter on score threashold or missing value\n        if len(filter_exp) > 0:\n                filter_exp += ' OR '\n        filter_exp += f'(({col_dict[col][\"SCORE_COL\"]} <= {score_val} ) OR ({col_dict[col][\"VAL_COL\"]} IS NULL))'\n\n    if len(filter_exp) > 0:\n       filter_exp = f'({filter_exp}) AND ' \n    \n    # Filter out documents already verified\n    filter_exp  += 'verification_date is null'\n    return filter_exp\n\n# Generates a column list for counting the number of documents that is missing values or a score less that the threashold\n# by each column\ndef count_missing_select(col_dict:dict, score_val:float):\n    select_list = []\n\n    for col in col_dict:\n        col_exp = (snow_funcs.sum(\n                          snow_funcs.iff(\n                                    (\n                                        (snow_funcs.col(col_dict[col][\"VAL_COL\"]).is_null())\n                                        | \n                                        (snow_funcs.col(col_dict[col][\"SCORE_COL\"]) <= score_val)\n                                    ), 1,0\n                              )\n                      ).as_(col)\n                )\n        select_list.append(col_exp)\n        \n    return select_list\n\n# Function to display a pdf page\ndef display_pdf_page():\n    pdf = st.session_state['pdf_doc']\n    page = pdf[st.session_state['pdf_page']]\n            \n    bitmap = page.render(\n                    scale = 8, \n                    rotation = 0,\n            )\n    pil_image = bitmap.to_pil()\n    st.image(pil_image)\n\n# Function to move to the next PDF page\ndef next_pdf_page():\n    if st.session_state.pdf_page + 1 >= len(st.session_state['pdf_doc']):\n        st.session_state.pdf_page = 0\n    else:\n        st.session_state.pdf_page += 1\n\n# Function to move to the previous PDF page\ndef previous_pdf_page():\n    if st.session_state.pdf_page > 0:\n        st.session_state.pdf_page -= 1\n\n# Function to get the name of all documents that need verification\ndef get_documents(doc_df):\n    \n    lst_docs = [dbRow[0] for dbRow in doc_df.collect()]\n    # Add a default None value\n    lst_docs.insert(0, None)\n    return lst_docs\n\n# MAIN\n\n# Get the table with all documents with extracted values\ndf_agreements = session.table(f\"{doc_ai_context}.{doc_ai_source_table}\")\n\n# Get the documents we already gave verified\ndf_validated_docs = session.table(f\"{doc_ai_context}.{doc_ai_source_verify_table}\")\n\n# Join\ndf_all_docs = df_agreements.join(df_validated_docs,on='RELATIVE_PATH', how='left', lsuffix = '_L', rsuffix = '_R')\n\n# Filter out all document that has missing values of score below the threasholds\nvalidate_filter = generate_filter(value_dict, threshold_score)\ndf_validate_docs = df_all_docs.filter(validate_filter)\n#col1, col2 = st.columns(2)\n#col1.metric(label=\"Total Documents\", value=df_agreements.count())\n#col2.metric(label=\"Documents Needing Validation\", value=df_validate_docs.count())\n\n# Get the number of documents by value that needs verifying\n#select_list = count_missing_select(value_dict, threshold_score)\n#df_verify_counts = df_validate_docs.select(select_list)\n#verify_cols = df_verify_counts.columns\n\n#st.subheader(\"Number of documents needing validation by extraction value\")\n#st.bar_chart(data=df_verify_counts.unpivot(\"needs_verify\", \"check_col\", verify_cols), x=\"CHECK_COL\", y=\"NEEDS_VERIFY\")\n\n# Verification section\nst.divider()\ncol1, col2 = st.columns(2)\nwith col1:\n    st.markdown('#### RAW PDF STORED IN FILE STORE')\n    with st.container():\n        # If we have selected a document\n        if file_id:        \n        # Display the extracted values\n            df_doc = df_validate_docs.filter(snow_funcs.col(\"FILE_NAME\") == file_id)\n            if 'pdf_page' not in st.session_state:\n                st.session_state['pdf_page'] = 0\n            if 'pdf_url' not in st.session_state:\n                st.session_state['pdf_url'] = file_id    \n            if 'pdf_doc' not in st.session_state or st.session_state['pdf_url'] != file_id:\n                pdf_stream = session.file.get_stream(f\"@{doc_ai_context}.{doc_ai_doc_stage}/{file_id}\")\n                pdf = pdfium.PdfDocument(pdf_stream)\n                st.session_state['pdf_doc'] = pdf\n                st.session_state['pdf_url'] = file_id\n                st.session_state['pdf_page'] = 0\n                \n            nav_col1, nav_col2, nav_col3 = st.columns(3)\n            with nav_col1:\n                if st.button(\"⏮️ Previous\", on_click=previous_pdf_page):\n                    pass    \n                with nav_col2:\n                    st.write(f\"page {st.session_state['pdf_page'] +1} of {len(st.session_state['pdf_doc'])} pages\")\n                with nav_col3:\n                    if st.button(\"Next ⏭️\", on_click=next_pdf_page):\n                        pass\n        \n    \n    \n            display_pdf_page()\n    with col2:\n        st.markdown('#### EXTRACTED TEXT FROM PDFS')\n        with st.container():\n            st.markdown(doc_detailsspd.CONTENT.iloc[0])\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51dfa611-fbb1-430c-8103-8589d00d3a5b",
   "metadata": {
    "name": "BROWSE_DOCS",
    "collapsed": false,
    "resultHeight": 113
   },
   "source": "### Chunk Documents to make the search service easier to find out information\nYou will see that the document parsing retains all the structure - making it easier to chop up into chunks for searching.  Also, it is fully aware of tables/paragraphs and headings  You will also note that additional fields have been created.  These can be additional slices to search the data."
  },
  {
   "cell_type": "code",
   "id": "41c73bc9-f938-4eb5-8522-c8309e1dae34",
   "metadata": {
    "language": "sql",
    "name": "chunked_table",
    "collapsed": false,
    "resultHeight": 0
   },
   "outputs": [],
   "source": "CREATE  TABLE IF NOT EXISTS DOCUMENT_AI.CHUNKED  AS \n\nSELECT \nRELATIVE_PATH,\nRATING,\nDATE_REPORT,\nNAME_OF_REPORT_PROVIDER,\nSPLIT_PART(RELATIVE_PATH,'/',1)::array DOCUMENT_TYPE,\nSPLIT_PART(RELATIVE_PATH,'/',2)::TEXT DOCUMENT, \n\n\n\nVALUE::TEXT TEXT FROM DOCUMENT_AI.REPORTS_ALL_DATA,LATERAL FLATTEN( SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(CONTENT,'markdown',500,20));\n\nSELECT * FROM DOCUMENT_AI.CHUNKED LIMIT 4",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a636914d-f38a-4bfe-8a00-50bb40634e6e",
   "metadata": {
    "name": "header_quarter_field",
    "collapsed": false,
    "resultHeight": 47
   },
   "source": "## Using cortex complete to help curate data further\n\nThis is an example where we wanted to a quarter/year field and the field information may not be 100% predictible.  Cortex Complete adds **reasoning** to the task - when it's difficult to complete task logically.  There is also a sentiment field included as well, which is simply using the cortex function **snowflake.cortex.sentiment**."
  },
  {
   "cell_type": "code",
   "id": "65170746-33e0-4d38-b671-c145b02e3b08",
   "metadata": {
    "language": "sql",
    "name": "CHUNKED_WITH_TIME",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS DOCUMENT_AI.ANALYST_REPORTS_CHUNKED AS\n\nSELECT *, REPLACE(SNOWFLAKE.CORTEX.COMPLETE('mistral-large', \nconcat('look at the following file name and return only Q2FY25 or Q3FY25.  September is in Q2 and november is in Q3', RELATIVE_PATH,'only return the answer in a format like this - Q3FY25. do not reurn comments' )),' ','')::ARRAY PERIOD, \nSNOWFLAKE.CORTEX.SENTIMENT(TEXT) SENTIMENT\n\n\nFROM DOCUMENT_AI.CHUNKED",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "326eb127-64b5-4c5b-b6ba-7b5d9a09e564",
   "metadata": {
    "language": "sql",
    "name": "VIEW_ALL_CHUNKED",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM DOCUMENT_AI.ANALYST_REPORTS_CHUNKED;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc5a9c4e-7447-454f-ad07-2a2e0c16a8d0",
   "metadata": {
    "name": "sound_transcripts",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "## Section 5 Analyse Sound Transcripts\n\nNavigate to **Projects > Notebooks** and begin the **ANALYSE_SOUND** notebook."
  },
  {
   "cell_type": "code",
   "id": "d032d9cc-748f-46e0-ad71-53ce177acc7b",
   "metadata": {
    "language": "sql",
    "name": "shared_transcripts",
    "resultHeight": 439,
    "collapsed": false
   },
   "outputs": [],
   "source": "SELECT * FROM DEFAULT_SCHEMA.SUMMARY_TEXT;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "872417b8-d830-42f6-87ef-a6d1c707293b",
   "metadata": {
    "language": "sql",
    "name": "sound_data",
    "collapsed": false,
    "resultHeight": 182
   },
   "outputs": [],
   "source": "CREATE VIEW IF NOT EXISTS DEFAULT_SCHEMA.EARNINGS_CALLS_FORMATTED AS \nSELECT RELATIVE_PATH, 'NA' RATING, 'NA' DATE_REPORT, 'NA' NAME_OF_REPORT_PROVIDER,  ['EARNINGS_CALLS'] DOCUMENT_TYPE,RELATIVE_PATH DOCUMENT, \nTEXT,REPLACE(REPLACE(REPLACE(REPLACE(RELATIVE_PATH,'EARNINGS_',''),'.mp3',''),'_',''),'FY20','FY')::ARRAY PERIOD, SENTIMENT FROM DEFAULT_SCHEMA.SUMMARY_TEXT;\n\nSELECT * FROM DEFAULT_SCHEMA.EARNINGS_CALLS_FORMATTED LIMIT 3",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "898dbe79-1555-4691-a2b7-cee1f144d10f",
   "metadata": {
    "language": "sql",
    "name": "union_text_sound",
    "collapsed": false,
    "resultHeight": 112
   },
   "outputs": [],
   "source": "CREATE TABLE IF NOT EXISTS DEFAULT_SCHEMA.TEXT_AND_SOUND AS \n\nselect * from DEFAULT_SCHEMA.EARNINGS_CALLS_FORMATTED\n\nUNION ALL\n\nSELECT * FROM DOCUMENT_AI.ANALYST_REPORTS_CHUNKED",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ac9b52fb-4b1b-4c82-be72-6ece4c19b892",
   "metadata": {
    "name": "cell3",
    "collapsed": false,
    "resultHeight": 46
   },
   "source": "### CREATE A SEARCH SERVICE"
  },
  {
   "cell_type": "code",
   "id": "0495d240-6d11-4faf-be9a-85eb05f0c92c",
   "metadata": {
    "language": "sql",
    "name": "viewing_single_chunk",
    "resultHeight": 0,
    "collapsed": false
   },
   "outputs": [],
   "source": "CREATE OR REPLACE  CORTEX SEARCH SERVICE DEFAULT_SCHEMA.CHUNKED_REPORTS\n  ON TEXT\n  ATTRIBUTES DOCUMENT_TYPE,PERIOD\n  WAREHOUSE = DEFAULT_WH\n  TARGET_LAG = '1 hour'\n  COMMENT = 'SEARCH SERVICE FOR REPORTS - CHUNKED'\n  AS SELECT * FROM DEFAULT_SCHEMA.TEXT_AND_SOUND;",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "63ee9eb8-5a47-4c68-b533-cb0c24080b9d",
   "metadata": {
    "language": "sql",
    "name": "view_search_service",
    "resultHeight": 112,
    "collapsed": false
   },
   "outputs": [],
   "source": "USE SCHEMA DEFAULT_SCHEMA;\nSHOW CORTEX SEARCH SERVICES",
   "execution_count": null
  }
 ]
}