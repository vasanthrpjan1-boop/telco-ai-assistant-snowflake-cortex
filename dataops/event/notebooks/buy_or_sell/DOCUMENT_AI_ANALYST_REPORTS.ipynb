{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c9ba374-cd85-415e-b3e5-a9e862c9d395",
   "metadata": {
    "collapsed": false,
    "name": "title",
    "resultHeight": 74
   },
   "source": [
    "# Document AI\n",
    "### Processing Analyst Reports\n",
    "\n",
    "You should now have built a model called **Analyst Reports** in document AI.  Now, the next stage is to process multiple documents using the model which you have built.\n",
    "\n",
    "Run the query below to view all the PDFs which are currently residing in a Snowflake Stage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643a6bb6-d83e-49ad-b950-6c9c52ce27a5",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "list_pdfs",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "select BUILD_SCOPED_FILE_URL('@DOCUMENT_AI.ANALYST_REPORTS',RELATIVE_PATH), * from directory(@DOCUMENT_AI.ANALYST_REPORTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1203b983-0dbd-44c0-937a-4c5fab4f42d5",
   "metadata": {
    "collapsed": false,
    "name": "process_docs",
    "resultHeight": 46
   },
   "source": [
    "### Process Documents from Document AI\n",
    "You will now use the model previously created to process these documents.  Each document will produce meta data under the column name **DOC_META** - this will consist of all the fields that was built in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33812c37-dbcb-46c0-88be-c3c37dd471c6",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "call_document_ai_model",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "CREATE TABLE if not exists DOCUMENT_AI.DOCUMENT_AI_PROCESSED AS\n",
    "SELECT\n",
    "\n",
    "*,\n",
    "\n",
    "DOCUMENT_AI.ANALYST_REPORTS!PREDICT(GET_PRESIGNED_URL(@DOCUMENT_AI.ANALYST_REPORTS,RELATIVE_PATH),1) DOC_META \n",
    "FROM DIRECTORY(@DOCUMENT_AI.ANALYST_REPORTS);\n",
    "\n",
    "SELECT * EXCLUDE (MD5, ETAG) FROM DOCUMENT_AI.DOCUMENT_AI_PROCESSED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e140c7-9ee5-46de-b5c4-971d620b7fb5",
   "metadata": {
    "collapsed": false,
    "name": "Format_the_view"
   },
   "source": [
    "Let's now make this more readable as a structured table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b416cc59-2b3e-4d8f-b3a9-8ac283eb9f68",
   "metadata": {
    "codeCollapsed": false,
    "collapsed": false,
    "language": "sql",
    "name": "view_meta_data",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCUMENT_AI.REPORTS_STRUCTURED AS \n",
    "select RELATIVE_PATH,\n",
    "\n",
    "DOC_META:__documentMetadata:ocrScore OCR_SCORE,\n",
    "DOC_META:RATING[0]:value::text RATING,\n",
    "DOC_META:MARKET_PRICE[0]:value::text MARKET_PRICE,\n",
    "DOC_META:DATE_REPORT[0]:value::text DATE_REPORT,\n",
    "DOC_META:NAME_OF_REPORT_PROVIDER[0]:value::text NAME_OF_REPORT_PROVIDER\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from DOCUMENT_AI.DOCUMENT_AI_PROCESSED;\n",
    "\n",
    "SELECT * FROM DOCUMENT_AI.REPORTS_STRUCTURED"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1a0c7b-8ed2-4fa1-acf0-b02ba5b51123",
   "metadata": {
    "collapsed": false,
    "name": "heading_extract_data",
    "resultHeight": 102
   },
   "source": [
    "## Extract ALL text from Analyst reports data\n",
    "We will use this to make the data searchable.  Document AI is good for taking out **key facts** about the data.  Cortex Parse Document extracts all the text - you can then later use this to make it searchable or run it through other AI function such as **sentiment**, **Summarize** or **Classification**.\n",
    "\n",
    "Run the following SQL below to parse the document\n",
    "\n",
    "Click [here](https://docs.snowflake.com/en/user-guide/snowflake-cortex/parse-document) for more information on Cortex Parse Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b896256-167f-40d2-a99d-a796b36f8ff8",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "extract_data",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCUMENT_AI.REPORTS AS \n",
    "\n",
    "select * exclude layout from (\n",
    "\n",
    "SELECT *, \n",
    "\n",
    "    build_stage_file_url(@DOCUMENT_AI.ANALYST_REPORTS,RELATIVE_PATH) URL,\n",
    "    \n",
    "    SNOWFLAKE.CORTEX.PARSE_DOCUMENT (\n",
    "                                '@DOCUMENT_AI.ANALYST_REPORTS',\n",
    "                                RELATIVE_PATH,\n",
    "                                {'mode': 'LAYOUT'} )  AS LAYOUT, LAYOUT:content::text CONTENT, LAYOUT:metadata:pageCount PAGE_COUNT\n",
    "        \n",
    "                                    FROM DIRECTORY (@DOCUMENT_AI.ANALYST_REPORTS));\n",
    "\n",
    "CREATE OR REPLACE VIEW DOCUMENT_AI.REPORTS_ALL_DATA AS\n",
    "\n",
    "select A.*, B.CONTENT from DOCUMENT_AI.REPORTS_STRUCTURED A INNER JOIN \n",
    "REPORTS B ON \n",
    "\n",
    "A.RELATIVE_PATH = B.RELATIVE_PATH;\n",
    "\n",
    "\n",
    "SELECT * FROM DOCUMENT_AI.REPORTS_ALL_DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3be0ad-f93e-4513-9675-1663b2ab5a34",
   "metadata": {
    "codeCollapsed": true,
    "collapsed": false,
    "language": "python",
    "name": "pdf_viewer",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "\n",
    "\n",
    "\n",
    "st.title(\"Equity Research Reports\")\n",
    "session = get_active_session()\n",
    "\n",
    "side_letters = session.table('DOCUMENT_AI.REPORTS_ALL_DATA').select('RELATIVE_PATH')#.filter(F.col('RELATIVE_PATH').like('ANALYST_REPORTS%'))\n",
    "file_id = st.selectbox('Select Report:', side_letters)\n",
    "doc_details = session.table('DOCUMENT_AI.REPORTS_ALL_DATA').filter(F.col('RELATIVE_PATH')==file_id).limit(1)\n",
    "doc_detailsspd = doc_details.to_pandas()\n",
    "\n",
    "\n",
    "st.markdown('#### Report Details')\n",
    "col1,col2 = st.columns(2)\n",
    "\n",
    "with col1:\n",
    "    st.markdown(f'''__Report Date:__ {doc_detailsspd.DATE_REPORT.iloc[0]}''')\n",
    "    st.markdown(f'''__Research Firm:__ {doc_detailsspd.NAME_OF_REPORT_PROVIDER.iloc[0]}''')\n",
    "    \n",
    "with col2:\n",
    "    st.markdown(f'''__Close Price Value:__ {doc_detailsspd.MARKET_PRICE.iloc[0]}''')\n",
    "    st.markdown(f'''__Recommendation:__ {doc_detailsspd.RATING.iloc[0]}''')\n",
    "\n",
    "# New Section \n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as snow_funcs\n",
    "\n",
    "import pypdfium2 as pdfium\n",
    "from datetime import datetime\n",
    "\n",
    "# Write directly to the app\n",
    "\n",
    "\n",
    "doc_ai_context = \"DATAOPS_EVENT_PROD.DOCUMENT_AI\"\n",
    "doc_ai_source_table = \"REPORTS\"\n",
    "doc_ai_source_verify_table = \"REPORTS_ALL_DATA\"\n",
    "doc_ai_doc_stage = \"ANALYST_REPORTS\"\n",
    "\n",
    "# Dict that has the name of the columns that needs to be verified, it has the column name of the column \n",
    "# with value and column with the score\n",
    "value_dict = {\n",
    "    \"OPERATOR_VALUE\": {\n",
    "        \"VAL_COL\": \"OPERATOR_VALUE\",\n",
    "        \"SCORE_COL\": \"OPERATOR_SCORE\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# The minimum score needed to not be verified\n",
    "threshold_score = 0.5\n",
    "\n",
    "# HELPER FUNCTIONS\n",
    "# Function to generate filter to only get the rows that are missing values or have a score below the threshold\n",
    "def generate_filter(col_dict:dict,  score_val:float): #score_cols:list, score_val:float, val_cols:list):\n",
    "    \n",
    "    filter_exp = ''\n",
    "\n",
    "    # For each column\n",
    "    for col in col_dict:\n",
    "        # Create the filter on score threashold or missing value\n",
    "        if len(filter_exp) > 0:\n",
    "                filter_exp += ' OR '\n",
    "        filter_exp += f'(({col_dict[col][\"SCORE_COL\"]} <= {score_val} ) OR ({col_dict[col][\"VAL_COL\"]} IS NULL))'\n",
    "\n",
    "    if len(filter_exp) > 0:\n",
    "       filter_exp = f'({filter_exp}) AND ' \n",
    "    \n",
    "    # Filter out documents already verified\n",
    "    filter_exp  += 'verification_date is null'\n",
    "    return filter_exp\n",
    "\n",
    "# Generates a column list for counting the number of documents that is missing values or a score less that the threashold\n",
    "# by each column\n",
    "def count_missing_select(col_dict:dict, score_val:float):\n",
    "    select_list = []\n",
    "\n",
    "    for col in col_dict:\n",
    "        col_exp = (snow_funcs.sum(\n",
    "                          snow_funcs.iff(\n",
    "                                    (\n",
    "                                        (snow_funcs.col(col_dict[col][\"VAL_COL\"]).is_null())\n",
    "                                        | \n",
    "                                        (snow_funcs.col(col_dict[col][\"SCORE_COL\"]) <= score_val)\n",
    "                                    ), 1,0\n",
    "                              )\n",
    "                      ).as_(col)\n",
    "                )\n",
    "        select_list.append(col_exp)\n",
    "        \n",
    "    return select_list\n",
    "\n",
    "# Function to display a pdf page\n",
    "def display_pdf_page():\n",
    "    pdf = st.session_state['pdf_doc']\n",
    "    page = pdf[st.session_state['pdf_page']]\n",
    "            \n",
    "    bitmap = page.render(\n",
    "                    scale = 8, \n",
    "                    rotation = 0,\n",
    "            )\n",
    "    pil_image = bitmap.to_pil()\n",
    "    st.image(pil_image)\n",
    "\n",
    "# Function to move to the next PDF page\n",
    "def next_pdf_page():\n",
    "    if st.session_state.pdf_page + 1 >= len(st.session_state['pdf_doc']):\n",
    "        st.session_state.pdf_page = 0\n",
    "    else:\n",
    "        st.session_state.pdf_page += 1\n",
    "\n",
    "# Function to move to the previous PDF page\n",
    "def previous_pdf_page():\n",
    "    if st.session_state.pdf_page > 0:\n",
    "        st.session_state.pdf_page -= 1\n",
    "\n",
    "# Function to get the name of all documents that need verification\n",
    "def get_documents(doc_df):\n",
    "    \n",
    "    lst_docs = [dbRow[0] for dbRow in doc_df.collect()]\n",
    "    # Add a default None value\n",
    "    lst_docs.insert(0, None)\n",
    "    return lst_docs\n",
    "\n",
    "# MAIN\n",
    "\n",
    "# Get the table with all documents with extracted values\n",
    "df_agreements = session.table(f\"{doc_ai_context}.{doc_ai_source_table}\")\n",
    "\n",
    "# Get the documents we already gave verified\n",
    "df_validated_docs = session.table(f\"{doc_ai_context}.{doc_ai_source_verify_table}\")\n",
    "\n",
    "# Join\n",
    "df_all_docs = df_agreements.join(df_validated_docs,on='RELATIVE_PATH', how='left', lsuffix = '_L', rsuffix = '_R')\n",
    "\n",
    "# Filter out all document that has missing values of score below the threasholds\n",
    "validate_filter = generate_filter(value_dict, threshold_score)\n",
    "df_validate_docs = df_all_docs.filter(validate_filter)\n",
    "#col1, col2 = st.columns(2)\n",
    "#col1.metric(label=\"Total Documents\", value=df_agreements.count())\n",
    "#col2.metric(label=\"Documents Needing Validation\", value=df_validate_docs.count())\n",
    "\n",
    "# Get the number of documents by value that needs verifying\n",
    "#select_list = count_missing_select(value_dict, threshold_score)\n",
    "#df_verify_counts = df_validate_docs.select(select_list)\n",
    "#verify_cols = df_verify_counts.columns\n",
    "\n",
    "#st.subheader(\"Number of documents needing validation by extraction value\")\n",
    "#st.bar_chart(data=df_verify_counts.unpivot(\"needs_verify\", \"check_col\", verify_cols), x=\"CHECK_COL\", y=\"NEEDS_VERIFY\")\n",
    "\n",
    "# Verification section\n",
    "st.divider()\n",
    "col1, col2 = st.columns(2)\n",
    "with col1:\n",
    "    st.markdown('#### RAW PDF STORED IN FILE STORE')\n",
    "    with st.container():\n",
    "        # If we have selected a document\n",
    "        if file_id:        \n",
    "        # Display the extracted values\n",
    "            df_doc = df_validate_docs.filter(snow_funcs.col(\"FILE_NAME\") == file_id)\n",
    "            if 'pdf_page' not in st.session_state:\n",
    "                st.session_state['pdf_page'] = 0\n",
    "            if 'pdf_url' not in st.session_state:\n",
    "                st.session_state['pdf_url'] = file_id    \n",
    "            if 'pdf_doc' not in st.session_state or st.session_state['pdf_url'] != file_id:\n",
    "                pdf_stream = session.file.get_stream(f\"@{doc_ai_context}.{doc_ai_doc_stage}/{file_id}\")\n",
    "                pdf = pdfium.PdfDocument(pdf_stream)\n",
    "                st.session_state['pdf_doc'] = pdf\n",
    "                st.session_state['pdf_url'] = file_id\n",
    "                st.session_state['pdf_page'] = 0\n",
    "                \n",
    "            nav_col1, nav_col2, nav_col3 = st.columns(3)\n",
    "            with nav_col1:\n",
    "                if st.button(\"⏮️ Previous\", on_click=previous_pdf_page):\n",
    "                    pass    \n",
    "                with nav_col2:\n",
    "                    st.write(f\"page {st.session_state['pdf_page'] +1} of {len(st.session_state['pdf_doc'])} pages\")\n",
    "                with nav_col3:\n",
    "                    if st.button(\"Next ⏭️\", on_click=next_pdf_page):\n",
    "                        pass\n",
    "        \n",
    "    \n",
    "    \n",
    "            display_pdf_page()\n",
    "    with col2:\n",
    "        st.markdown('#### EXTRACTED TEXT FROM PDFS')\n",
    "        with st.container(height=1000):\n",
    "            st.markdown(doc_detailsspd.CONTENT.iloc[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927d22c9-e114-4d20-91d6-67c277859ee6",
   "metadata": {
    "collapsed": false,
    "name": "summarize_h"
   },
   "source": [
    "### Cortex Summarize\n",
    "You will notice that some of these reports are very long.  Let's use Cortex Summarize to summarise them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498f4f73-bd65-4854-96f1-a9edd4e8d97a",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": [
    "SELECT NAME_OF_REPORT_PROVIDER, SNOWFLAKE.CORTEX.SUMMARIZE(CONTENT) SUMMARY FROM DOCUMENT_AI.REPORTS_ALL_DATA limit 5;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0232fe-5619-4b89-8d3f-ab1fb5016197",
   "metadata": {
    "language": "sql",
    "name": "cortex_summarize"
   },
   "outputs": [],
   "source": [
    "-- Formatting the summarised reports to use with the search service\n",
    "\n",
    "CREATE OR REPLACE TABLE DOCUMENT_AI.SUMMARISED_ANALYST_REPORTS AS \n",
    "\n",
    "\n",
    "SELECT \n",
    "RELATIVE_PATH,\n",
    "RATING,\n",
    "DATE_REPORT,\n",
    "NAME_OF_REPORT_PROVIDER,\n",
    "['ANALYST_REPORTS'] DOCUMENT_TYPE, \n",
    "SPLIT_PART(RELATIVE_PATH,'/',1)::text DOCUMENT,\n",
    "SUMMARY TEXT\n",
    "\n",
    "FROM\n",
    "\n",
    "\n",
    "(\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "SELECT * EXCLUDE CONTENT,SNOWFLAKE.CORTEX.SUMMARIZE(CONTENT) SUMMARY FROM DOCUMENT_AI.REPORTS_ALL_DATA);\n",
    "\n",
    "SELECT * FROM DOCUMENT_AI.SUMMARISED_ANALYST_REPORTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a94c45-9f2f-40b7-8226-cbf54d6e79a0",
   "metadata": {
    "collapsed": false,
    "name": "view_summary_h"
   },
   "source": [
    "And this is how we can view each summary in streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a113a7b-d216-4253-9dc3-42969b388bf1",
   "metadata": {
    "codeCollapsed": false,
    "language": "python",
    "name": "view_summarized_reports"
   },
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "import streamlit as st\n",
    "from snowflake.snowpark.context import get_active_session\n",
    "from snowflake.snowpark import functions as F\n",
    "from snowflake.snowpark import types as T\n",
    "session = get_active_session()\n",
    "st.markdown('#### Summarised Reports')\n",
    "selected_summary = st.radio('Select Summary:',\n",
    "            cortex_summarize.to_df().select('RELATIVE_PATH').distinct())\n",
    "\n",
    "data = cortex_summarize.to_df().filter(F.col('RELATIVE_PATH')==selected_summary)\n",
    "\n",
    "st.markdown(data.to_pandas().TEXT.iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51dfa611-fbb1-430c-8103-8589d00d3a5b",
   "metadata": {
    "collapsed": false,
    "name": "BROWSE_DOCS",
    "resultHeight": 113
   },
   "source": [
    "### Chunk Documents to make the search service easier to find out information\n",
    "\n",
    "Let's go back to the original long document.  Rather than creating a summary where you lose some detail, you can chunk all the data to make it available for analysis within a search service.\n",
    "\n",
    "You will see that the document parsing retains all the structure - making it easier to chop up into chunks for searching.  Also, it is fully aware of tables/paragraphs and headings  You will also note that additional fields have been created.  These can be additional slices to search the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c73bc9-f938-4eb5-8522-c8309e1dae34",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "chunked_table",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCUMENT_AI.CHUNKED  AS \n",
    "\n",
    "SELECT \n",
    "RELATIVE_PATH,\n",
    "RATING,\n",
    "DATE_REPORT,\n",
    "NAME_OF_REPORT_PROVIDER,\n",
    "['ANALYST_REPORTS'] DOCUMENT_TYPE, \n",
    "SPLIT_PART(RELATIVE_PATH,'/',1)::text DOCUMENT,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "VALUE::TEXT TEXT FROM DOCUMENT_AI.REPORTS_ALL_DATA,LATERAL FLATTEN( SNOWFLAKE.CORTEX.SPLIT_TEXT_RECURSIVE_CHARACTER(CONTENT,'none',600,50,['\\n\\n', ' ']));\n",
    "\n",
    "SELECT * FROM DOCUMENT_AI.CHUNKED LIMIT 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a636914d-f38a-4bfe-8a00-50bb40634e6e",
   "metadata": {
    "collapsed": false,
    "name": "header_quarter_field",
    "resultHeight": 47
   },
   "source": [
    "## Using cortex complete to help curate data further\n",
    "\n",
    "This is an example where we wanted to a quarter/year field and the field information may not be 100% predictible.  Cortex Complete adds **reasoning** to the task - when it's difficult to complete task logically.  There is also a sentiment field included as well, which is simply using the cortex function **snowflake.cortex.sentiment**.\n",
    "\n",
    "Summary and Detailed information is included in the search which helps answer the questions if 'overview' time questions are asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65170746-33e0-4d38-b671-c145b02e3b08",
   "metadata": {
    "collapsed": false,
    "language": "sql",
    "name": "CHUNKED_WITH_TIME",
    "resultHeight": 0
   },
   "outputs": [],
   "source": [
    "CREATE OR REPLACE TABLE DOCUMENT_AI.ANALYST_REPORTS_CHUNKED AS\n",
    "\n",
    "SELECT *, REPLACE(SNOWFLAKE.CORTEX.COMPLETE('mistral-large', \n",
    "concat('look at the following file name and return only Q2FY25 or Q3FY25.  September is in Q2 and november is in Q3', RELATIVE_PATH,'only return the answer in a format like this - Q3FY25. do not reurn comments' )),' ','')::ARRAY PERIOD, \n",
    "SNOWFLAKE.CORTEX.SENTIMENT(TEXT) SENTIMENT\n",
    "\n",
    "\n",
    "FROM \n",
    "\n",
    "(SELECT 'DETAILED' AGGREGATION, * FROM DOCUMENT_AI.CHUNKED UNION SELECT 'SUMMARY' AGGREGATION, * FROM DOCUMENT_AI.SUMMARISED_ANALYST_REPORTS);\n",
    "\n",
    "SELECT * FROM DOCUMENT_AI.ANALYST_REPORTS_CHUNKED LIMIT 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9362d9ca-e9d7-4706-847a-8e17d33dc149",
   "metadata": {
    "collapsed": false,
    "name": "CONCLUSION"
   },
   "source": [
    "You have now produced meta data as well as chunking unstructured analyst report data to gain better insights of Snowflake Stock.  Now, Let's process the infographic information.  This dataset will hold key metrics and will not contain large text extracts.  \n",
    "\n",
    "- Go back to the notebooks in the project area and click on **DOCUMENTAI_INFOGRAPHICS**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "authorEmail": "",
   "authorId": "8713096050558",
   "authorName": "USER",
   "lastEditTime": 1744292644632,
   "notebookId": "cuv4mtebbfs3yyrtjun6",
   "sessionId": "d1f5daa9-1d06-4778-8ef4-13358df29504"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
